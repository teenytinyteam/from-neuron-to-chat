{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Shortcut Connection",
   "id": "e50329ad2029b820"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.541058Z",
     "start_time": "2025-09-07T22:38:42.410092Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "id": "af724e9e3d22b7f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.575293Z",
     "start_time": "2025-09-07T22:38:42.556457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = None\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def gradient(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.gradient()\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __add__(self, other):\n",
    "        p = Tensor(self.data + other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        p = Tensor(self.data - other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = -p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        p = Tensor(self.data * other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad * other.data\n",
    "            other.grad = p.grad * self.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        p = Tensor(self.data / other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad / other.data\n",
    "            other.grad = -p.grad * self.data / (other.data ** 2)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        p = Tensor(np.matmul(self.data, other.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = np.matmul(p.grad, other.data.swapaxes(-1, -2))\n",
    "            other.grad = np.matmul(self.data.swapaxes(-1, -2), p.grad)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def transpose(self, axes=None):\n",
    "        p = Tensor(np.transpose(self.data, axes))\n",
    "\n",
    "        def gradient_fn():\n",
    "            if axes is None:\n",
    "                self.grad = np.transpose(p.grad)\n",
    "            else:\n",
    "                idx = np.argsort(axes)\n",
    "                self.grad = np.transpose(p.grad, idx)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self}\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.transpose()\n",
    "\n",
    "    def concat(self, other, axis):\n",
    "        p = Tensor(np.concatenate([self.data, other.data], axis=axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad, other.grad = np.split(p.grad, [self.data.shape[axis]], axis=axis)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        p = Tensor(np.reshape(self.data, shape))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = np.reshape(p.grad, self.data.shape)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self}\n",
    "        return p"
   ],
   "id": "b94392b1a8ee3238",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.611749Z",
     "start_time": "2025-09-07T22:38:42.603197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, filename, batch_size, stride):\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read().lower()\n",
    "\n",
    "        self.vocabulary = sorted(set(self.split_text(self.text)))\n",
    "        self.vocabulary.extend(['<|eos|>', '<|unk|>'])\n",
    "        self.word2index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.index2word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.tokens = self.encode(self.text)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def split_text(text):\n",
    "        words = re.split(r'([,.:;?_!\"()\\']|\\s)', text.lower())\n",
    "        return [t.strip() for t in words if t.strip()]\n",
    "\n",
    "    def train(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for i in range(0, len(self.tokens) * 9 // 10 - self.batch_size,\n",
    "                       self.stride):\n",
    "            self.features.append(self.tokens[i: i + self.batch_size])\n",
    "            self.labels.append(self.tokens[i + 1: i + self.batch_size + 1])\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for i in range(len(self.tokens) * 9 // 10 - self.batch_size + 1,\n",
    "                       len(self.tokens) - self.batch_size,\n",
    "                       self.stride):\n",
    "            self.features.append(self.tokens[i: i + self.batch_size])\n",
    "            self.labels.append(self.tokens[i + 1: i + self.batch_size + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.split_text(text)\n",
    "        words = [word if word in self.word2index else '<|unk|>' for word in words]\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.index2word[index] for index in tokens])\n",
    "        return re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "\n",
    "    def embedding(self, index):\n",
    "        ebd = np.zeros(len(self.vocabulary))\n",
    "        ebd[index] = 1\n",
    "        return ebd"
   ],
   "id": "dcc78e777ad99d7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.669456Z",
     "start_time": "2025-09-07T22:38:42.662668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ],
   "id": "aab403b547fa3d05",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.719940Z",
     "start_time": "2025-09-07T22:38:42.712144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(out_size, in_size) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad = p.grad.T @ x.data\n",
    "            self.bias.grad = np.sum(p.grad, axis=0)\n",
    "            x.grad = p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight, self.bias, x}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ],
   "id": "8dc7737c0e9d9ca1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.769471Z",
     "start_time": "2025-09-07T22:38:42.764150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size, axis=None):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.axis = axis\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(embedding_size, vocabulary_size) / vocabulary_size)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        weights = self.weight.data.T[x.data]\n",
    "        p = Tensor(np.sum(weights, axis=self.axis) if self.axis is not None else weights)\n",
    "\n",
    "        def gradient_fn():\n",
    "            if self.weight.grad is None:\n",
    "                self.weight.grad = np.zeros_like(self.weight.data)\n",
    "            self.weight.grad.T[x.data] += p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]"
   ],
   "id": "9053901426a9d8e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.822531Z",
     "start_time": "2025-09-07T22:38:42.814081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Normalize(Layer):\n",
    "\n",
    "    def __init__(self, size, eps=0.00001):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.eps = eps\n",
    "\n",
    "        self.scale = Tensor(np.ones(self.size))\n",
    "        self.shift = Tensor(np.zeros(self.size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        mean = np.mean(x.data, axis=-1, keepdims=True)\n",
    "        var = np.var(x.data, axis=-1, keepdims=True, ddof=0)\n",
    "        norm = (x.data - mean) / np.sqrt(var + self.eps)\n",
    "        p = Tensor(self.scale.data * norm + self.shift.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            axis = tuple(range(p.grad.ndim - 1))\n",
    "            self.scale.grad = np.sum(p.grad * norm, axis=axis)\n",
    "            self.shift.grad = np.sum(p.grad, axis=axis)\n",
    "\n",
    "            grad = p.grad * self.scale.data\n",
    "            grad_mean = np.mean(grad, axis=-1, keepdims=True)\n",
    "            norm_mean = np.mean(grad * norm, axis=-1, keepdims=True)\n",
    "            x.grad = (grad - grad_mean - norm * norm_mean) / np.sqrt(var + self.eps)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.scale, self.shift, x}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.scale, self.shift]"
   ],
   "id": "178e90b0bde8d20e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.874163Z",
     "start_time": "2025-09-07T22:38:42.867420Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Triu(Layer):\n",
    "\n",
    "    def __init__(self, value=-np.inf):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        axes = list(range(x.data.ndim))\n",
    "        axes[-2], axes[-1] = axes[-1], axes[-2]\n",
    "        mask = np.triu(np.ones(x.data.shape)).transpose(axes)\n",
    "        p = Tensor(x.data)\n",
    "        p.data[mask == 0] = self.value\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * mask\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "f12fa65f7a7cf5ed",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.925597Z",
     "start_time": "2025-09-07T22:38:42.919876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GeLU(Layer):\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        sqrt_2_over_pi = np.sqrt(2 / np.pi)\n",
    "        tanh_input = sqrt_2_over_pi * (x.data + 0.044715 * x.data ** 3)\n",
    "        gelu_output = 0.5 * x.data * (1 + np.tanh(tanh_input))\n",
    "\n",
    "        p = Tensor(gelu_output)\n",
    "\n",
    "        def gradient_fn():\n",
    "            tanh_val = np.tanh(tanh_input)\n",
    "            sech2_val = 1 - tanh_val ** 2\n",
    "            dtanh_input_dx = sqrt_2_over_pi * (1 + 3 * 0.044715 * x.data ** 2)\n",
    "            gelu_grad = 0.5 * (1 + tanh_val) + 0.5 * x.data * sech2_val * dtanh_input_dx\n",
    "            x.grad = gelu_grad * p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "297b7bba53b9e161",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:42.983227Z",
     "start_time": "2025-09-07T22:38:42.973385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def __init__(self, axis=-1):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        exp = np.exp(x.data - np.max(x.data, axis=self.axis, keepdims=True))\n",
    "        p = Tensor(exp / np.sum(exp, axis=self.axis, keepdims=True))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = np.zeros_like(x.data)\n",
    "\n",
    "            shape = x.data.shape\n",
    "            axis = self.axis if self.axis >= 0 else len(shape) + self.axis\n",
    "            shapes = list(shape)\n",
    "            shapes.pop(axis)\n",
    "\n",
    "            for idx in np.ndindex(tuple(shapes)):\n",
    "                indices = list(idx)\n",
    "                indices.insert(axis, slice(None))\n",
    "                indices = tuple(indices)\n",
    "\n",
    "                probs = p.data[indices]\n",
    "                grad = p.grad[indices]\n",
    "\n",
    "                probs_col = probs.reshape(-1, 1)\n",
    "                jacobian = np.diagflat(probs) - probs_col @ probs_col.T\n",
    "                x.grad[indices] = jacobian @ grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "85efcf24910684e0",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.086158Z",
     "start_time": "2025-09-07T22:38:43.078404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CELoss:\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        exp = np.exp(p.data - np.max(p.data, axis=-1, keepdims=True))\n",
    "        softmax = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "        log = np.log(softmax + 1e-10)\n",
    "        ce = Tensor(0 - np.sum(y.data * log) / len(p.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad = (softmax - y.data) / len(p.data)\n",
    "\n",
    "        ce.gradient_fn = gradient_fn\n",
    "        ce.parents = {p}\n",
    "        return ce"
   ],
   "id": "b5186f5070a37fed",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.125868Z",
     "start_time": "2025-09-07T22:38:43.116253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = [None for _ in range(len(params))]\n",
    "        self.v = [None for _ in range(len(params))]\n",
    "        self.t = 0\n",
    "\n",
    "    def backward(self):\n",
    "        self.t += 1\n",
    "        for idx, p in enumerate(self.parameters):\n",
    "            if p is not None and p.grad is not None:\n",
    "                grad = p.grad.reshape(p.data.shape)\n",
    "\n",
    "                if self.m[idx] is None:\n",
    "                    self.m[idx] = np.zeros_like(p.data)\n",
    "                    self.v[idx] = np.zeros_like(p.data)\n",
    "\n",
    "                self.m[idx] = self.beta1 * self.m[idx] + (1 - self.beta1) * grad\n",
    "                self.v[idx] = self.beta2 * self.v[idx] + (1 - self.beta2) * (grad ** 2)\n",
    "                m_hat = self.m[idx] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[idx] / (1 - self.beta2 ** self.t)\n",
    "                p.data -= m_hat / (np.sqrt(v_hat) + self.eps) * self.lr"
   ],
   "id": "2c553264da7a502e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.173570Z",
     "start_time": "2025-09-07T22:38:43.169655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTEmbedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, context_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.token_embedding = Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.positional_embedding = Embedding(self.context_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.token_embedding,\n",
    "                       self.positional_embedding]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        token = self.token_embedding(x)\n",
    "        position = self.positional_embedding(Tensor(range(self.context_size)))\n",
    "        return token + position"
   ],
   "id": "100377392ef699b1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.226611Z",
     "start_time": "2025-09-07T22:38:43.218274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTAttention(Layer):\n",
    "\n",
    "    def __init__(self, context_size, embedding_size, heads=1):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "\n",
    "        self.normalize = Normalize(self.embedding_size)\n",
    "        self.attention_query = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.attention_key = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.attention_value = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.triu = Triu(self.context_size)\n",
    "        self.softmax = Softmax()\n",
    "        self.merge = Linear(self.heads * self.embedding_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.normalize,\n",
    "                       self.attention_query,\n",
    "                       self.attention_key,\n",
    "                       self.attention_value,\n",
    "                       self.triu,\n",
    "                       self.softmax,\n",
    "                       self.merge]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        norm = self.normalize(x)\n",
    "\n",
    "        query = self.attention_query(norm).reshape((-1, self.heads, self.embedding_size))\n",
    "        key = self.attention_key(norm).reshape((-1, self.heads, self.embedding_size))\n",
    "        value = self.attention_value(norm).reshape((-1, self.heads, self.embedding_size))\n",
    "\n",
    "        scores = self.triu(query @ key.transpose((0, 2, 1)))\n",
    "        weights = self.softmax(scores)\n",
    "        vectors = self.merge((weights @ value).reshape((-1, self.heads * self.embedding_size)))\n",
    "        return x + vectors"
   ],
   "id": "97be38c6e45f888d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.281969Z",
     "start_time": "2025-09-07T22:38:43.274783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTFeedForward(Layer):\n",
    "\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.normalize = Normalize(self.embedding_size)\n",
    "        self.input = Linear(self.embedding_size, self.embedding_size * 4)\n",
    "        self.gelu = GeLU()\n",
    "        self.output = Linear(self.embedding_size * 4, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.normalize,\n",
    "                       self.input,\n",
    "                       self.gelu,\n",
    "                       self.output]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        norm = self.normalize(x)\n",
    "        hidden = self.gelu(self.input(norm))\n",
    "        return x + self.output(hidden)"
   ],
   "id": "881fc2cb691e00df",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.334477Z",
     "start_time": "2025-09-07T22:38:43.327245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTOutput(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.normalize = Normalize(self.embedding_size)\n",
    "        self.output = Linear(self.embedding_size, self.vocabulary_size)\n",
    "\n",
    "        self.layers = [self.normalize,\n",
    "                       self.output]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        norm = self.normalize(x)\n",
    "        return self.output(norm)"
   ],
   "id": "8d5e201554a475ad",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.389675Z",
     "start_time": "2025-09-07T22:38:43.380882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, context_size, embedding_size, heads=1):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "\n",
    "        self.embedding = GPTEmbedding(self.vocabulary_size, self.context_size, self.embedding_size)\n",
    "        self.attention = GPTAttention(self.context_size, self.embedding_size, self.heads)\n",
    "        self.feed_forward = GPTFeedForward(self.embedding_size)\n",
    "        self.output = GPTOutput(self.vocabulary_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.embedding,\n",
    "                       self.attention,\n",
    "                       self.feed_forward,\n",
    "                       self.output]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.feed_forward(x)\n",
    "        return self.output(x)"
   ],
   "id": "dacb882228927e2e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:43.441501Z",
     "start_time": "2025-09-07T22:38:43.436253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_SIZE = 3\n",
    "HEADS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10"
   ],
   "id": "3d70da68cae62a54",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:54.953958Z",
     "start_time": "2025-09-07T22:38:43.488314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DataLoader('../one-day.txt', CONTEXT_SIZE, 1)\n",
    "\n",
    "model = GPT(len(dataset.vocabulary), CONTEXT_SIZE, EMBEDDING_SIZE, HEADS)\n",
    "loss = CELoss()\n",
    "sgd = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(len(dataset)):\n",
    "        feature, label = dataset[i]\n",
    "\n",
    "        prediction = model(Tensor(feature))\n",
    "        error = loss(prediction, Tensor(dataset.embedding(label)))\n",
    "        error.gradient()\n",
    "        sgd.backward()\n",
    "\n",
    "print(f'Prediction: {prediction.data}')\n",
    "print(f'Error: {error.data}')"
   ],
   "id": "ae9066aafc2cb5f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[-0.35795329  0.18240149  0.27687973 ... -0.04033078 -0.211052\n",
      "   0.2068304 ]\n",
      " [ 0.35922572 -0.22610391 -0.29132764 ... -0.04387742  0.18401592\n",
      "  -0.21390163]\n",
      " [-0.27334746  0.08837217  0.19551393 ... -0.13055473 -0.19402862\n",
      "   0.15045082]\n",
      " [ 0.33545198 -0.23418363 -0.27925346 ... -0.08612025  0.15696586\n",
      "  -0.20313668]]\n",
      "Error: 22.374767085983777\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T22:38:55.036426Z",
     "start_time": "2025-09-07T22:38:54.983628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset.eval()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    feature, label = dataset[i]\n",
    "\n",
    "    print(\"Feature: \", dataset.decode(feature))\n",
    "    print(\"Label: \", dataset.decode(label))\n",
    "\n",
    "    prediction = model(Tensor(feature))\n",
    "    prediction_tokens = []\n",
    "    for j in range(len(label)):\n",
    "        prediction_tokens.append(prediction.data[j].argmax())\n",
    "    print(\"Prediction: \", dataset.decode(prediction_tokens))"
   ],
   "id": "571f2c7fa791951",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  about his day.\n",
      "Label:  his day. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his day. he\n",
      "Label:  day. he shows\n",
      "Prediction:  tall had hops had\n",
      "Feature:  day. he shows\n",
      "Label:  . he shows his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he shows his\n",
      "Label:  he shows his notebook\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he shows his notebook\n",
      "Label:  shows his notebook.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  shows his notebook.\n",
      "Label:  his notebook.\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his notebook.\"\n",
      "Label:  notebook.\" you\n",
      "Prediction:  tall had hops had\n",
      "Feature:  notebook.\" you\n",
      "Label:  .\" you had\n",
      "Prediction:  tall had hops had\n",
      "Feature:  .\" you had\n",
      "Label:  \" you had quite\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" you had quite\n",
      "Label:  you had quite an\n",
      "Prediction:  tall had hops had\n",
      "Feature:  you had quite an\n",
      "Label:  had quite an adventure\n",
      "Prediction:  tall had hops had\n",
      "Feature:  had quite an adventure\n",
      "Label:  quite an adventure,\n",
      "Prediction:  tall had hops had\n",
      "Feature:  quite an adventure,\n",
      "Label:  an adventure,\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  an adventure,\"\n",
      "Label:  adventure,\" his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  adventure,\" his\n",
      "Label:  ,\" his father\n",
      "Prediction:  tall had hops had\n",
      "Feature:  ,\" his father\n",
      "Label:  \" his father says\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" his father says\n",
      "Label:  his father says.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his father says.\n",
      "Label:  father says.\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  father says.\"\n",
      "Label:  says.\" yes\n",
      "Prediction:  tall had hops had\n",
      "Feature:  says.\" yes\n",
      "Label:  .\" yes,\n",
      "Prediction:  tall had hops had\n",
      "Feature:  .\" yes,\n",
      "Label:  \" yes,\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" yes,\"\n",
      "Label:  yes,\" tom\n",
      "Prediction:  tall had hops had\n",
      "Feature:  yes,\" tom\n",
      "Label:  ,\" tom smiles\n",
      "Prediction:  tall had hops had\n",
      "Feature:  ,\" tom smiles\n",
      "Label:  \" tom smiles.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" tom smiles.\n",
      "Label:  tom smiles. night\n",
      "Prediction:  tall had hops had\n",
      "Feature:  tom smiles. night\n",
      "Label:  smiles. night falls\n",
      "Prediction:  tall had hops had\n",
      "Feature:  smiles. night falls\n",
      "Label:  . night falls after\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . night falls after\n",
      "Label:  night falls after dinner\n",
      "Prediction:  tall had hops had\n",
      "Feature:  night falls after dinner\n",
      "Label:  falls after dinner,\n",
      "Prediction:  tall had hops had\n",
      "Feature:  falls after dinner,\n",
      "Label:  after dinner, tom\n",
      "Prediction:  tall had hops had\n",
      "Feature:  after dinner, tom\n",
      "Label:  dinner, tom takes\n",
      "Prediction:  tall had hops had\n",
      "Feature:  dinner, tom takes\n",
      "Label:  , tom takes a\n",
      "Prediction:  tall had hops had\n",
      "Feature:  , tom takes a\n",
      "Label:  tom takes a bath\n",
      "Prediction:  tall had hops had\n",
      "Feature:  tom takes a bath\n",
      "Label:  takes a bath.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  takes a bath.\n",
      "Label:  a bath. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  a bath. he\n",
      "Label:  bath. he puts\n",
      "Prediction:  tall had hops had\n",
      "Feature:  bath. he puts\n",
      "Label:  . he puts on\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he puts on\n",
      "Label:  he puts on his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he puts on his\n",
      "Label:  puts on his pajamas\n",
      "Prediction:  tall had hops had\n",
      "Feature:  puts on his pajamas\n",
      "Label:  on his pajamas.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  on his pajamas.\n",
      "Label:  his pajamas. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his pajamas. he\n",
      "Label:  pajamas. he brushes\n",
      "Prediction:  tall had hops had\n",
      "Feature:  pajamas. he brushes\n",
      "Label:  . he brushes his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he brushes his\n",
      "Label:  he brushes his teeth\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he brushes his teeth\n",
      "Label:  brushes his teeth.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  brushes his teeth.\n",
      "Label:  his teeth. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his teeth. he\n",
      "Label:  teeth. he lies\n",
      "Prediction:  tall had hops had\n",
      "Feature:  teeth. he lies\n",
      "Label:  . he lies in\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he lies in\n",
      "Label:  he lies in bed\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he lies in bed\n",
      "Label:  lies in bed and\n",
      "Prediction:  tall had hops had\n",
      "Feature:  lies in bed and\n",
      "Label:  in bed and looks\n",
      "Prediction:  tall had hops had\n",
      "Feature:  in bed and looks\n",
      "Label:  bed and looks at\n",
      "Prediction:  tall had hops had\n",
      "Feature:  bed and looks at\n",
      "Label:  and looks at the\n",
      "Prediction:  tall had hops had\n",
      "Feature:  and looks at the\n",
      "Label:  looks at the ceiling\n",
      "Prediction:  tall had hops had\n",
      "Feature:  looks at the ceiling\n",
      "Label:  at the ceiling.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  at the ceiling.\n",
      "Label:  the ceiling. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  the ceiling. he\n",
      "Label:  ceiling. he thinks\n",
      "Prediction:  tall had hops had\n",
      "Feature:  ceiling. he thinks\n",
      "Label:  . he thinks about\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he thinks about\n",
      "Label:  he thinks about his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he thinks about his\n",
      "Label:  thinks about his day\n",
      "Prediction:  tall had hops had\n",
      "Feature:  thinks about his day\n",
      "Label:  about his day.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  about his day.\n",
      "Label:  his day. he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his day. he\n",
      "Label:  day. he whispers\n",
      "Prediction:  tall had hops had\n",
      "Feature:  day. he whispers\n",
      "Label:  . he whispers,\n",
      "Prediction:  tall had hops had\n",
      "Feature:  . he whispers,\n",
      "Label:  he whispers,\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he whispers,\"\n",
      "Label:  whispers,\" thank\n",
      "Prediction:  tall had hops had\n",
      "Feature:  whispers,\" thank\n",
      "Label:  ,\" thank you\n",
      "Prediction:  tall had hops had\n",
      "Feature:  ,\" thank you\n",
      "Label:  \" thank you for\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" thank you for\n",
      "Label:  thank you for the\n",
      "Prediction:  tall had hops had\n",
      "Feature:  thank you for the\n",
      "Label:  you for the adventure\n",
      "Prediction:  tall had hops had\n",
      "Feature:  you for the adventure\n",
      "Label:  for the adventure.\n",
      "Prediction:  tall had hops had\n",
      "Feature:  for the adventure.\n",
      "Label:  the adventure.\"\n",
      "Prediction:  tall had hops had\n",
      "Feature:  the adventure.\"\n",
      "Label:  adventure.\" he\n",
      "Prediction:  tall had hops had\n",
      "Feature:  adventure.\" he\n",
      "Label:  .\" he closes\n",
      "Prediction:  tall had hops had\n",
      "Feature:  .\" he closes\n",
      "Label:  \" he closes his\n",
      "Prediction:  tall had hops had\n",
      "Feature:  \" he closes his\n",
      "Label:  he closes his eyes\n",
      "Prediction:  tall had hops had\n",
      "Feature:  he closes his eyes\n",
      "Label:  closes his eyes and\n",
      "Prediction:  tall had hops had\n",
      "Feature:  closes his eyes and\n",
      "Label:  his eyes and falls\n",
      "Prediction:  tall had hops had\n",
      "Feature:  his eyes and falls\n",
      "Label:  eyes and falls asleep\n",
      "Prediction:  tall had hops had\n",
      "Feature:  eyes and falls asleep\n",
      "Label:  and falls asleep.\n",
      "Prediction:  tall had hops had\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
