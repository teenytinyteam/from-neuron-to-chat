{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Text Generation",
   "id": "93bb4a88d9adc84d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:14.702669Z",
     "start_time": "2025-09-07T20:43:14.699424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "id": "24cff6dbe303bf4d",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:14.767029Z",
     "start_time": "2025-09-07T20:43:14.751247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = None\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def gradient(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.gradient()\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __add__(self, other):\n",
    "        p = Tensor(self.data + other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        p = Tensor(self.data - other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = -p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        p = Tensor(self.data * other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad * other.data\n",
    "            other.grad = p.grad * self.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        p = Tensor(self.data / other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad / other.data\n",
    "            other.grad = -p.grad * self.data / (other.data ** 2)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        p = Tensor(np.matmul(self.data, other.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = np.matmul(p.grad, other.data.swapaxes(-1, -2))\n",
    "            other.grad = np.matmul(self.data.swapaxes(-1, -2), p.grad)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def transpose(self, axes=None):\n",
    "        p = Tensor(np.transpose(self.data, axes))\n",
    "\n",
    "        def gradient_fn():\n",
    "            if axes is None:\n",
    "                self.grad = np.transpose(p.grad)\n",
    "            else:\n",
    "                idx = np.argsort(axes)\n",
    "                self.grad = np.transpose(p.grad, idx)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self}\n",
    "        return p\n",
    "\n",
    "    @property\n",
    "    def T(self):\n",
    "        return self.transpose()\n",
    "\n",
    "    def concat(self, other, axis):\n",
    "        p = Tensor(np.concatenate([self.data, other.data], axis=axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad, other.grad = np.split(p.grad, [self.data.shape[axis]], axis=axis)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def reshape(self, shape):\n",
    "        p = Tensor(np.reshape(self.data, shape))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = np.reshape(p.grad, self.data.shape)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self}\n",
    "        return p"
   ],
   "id": "731de11e9bd2b1f1",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:14.817708Z",
     "start_time": "2025-09-07T20:43:14.805955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, filename, batch_size, stride):\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "\n",
    "        with open(self.filename, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read().lower()\n",
    "\n",
    "        self.vocabulary = sorted(set(self.split_text(self.text)))\n",
    "        self.vocabulary.extend(['<|eos|>', '<|unk|>'])\n",
    "        self.word2index = {word: index for index, word in enumerate(self.vocabulary)}\n",
    "        self.index2word = {index: word for index, word in enumerate(self.vocabulary)}\n",
    "        self.tokens = self.encode(self.text)\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def split_text(text):\n",
    "        words = re.split(r'([,.:;?_!\"()\\']|\\s)', text.lower())\n",
    "        return [t.strip() for t in words if t.strip()]\n",
    "\n",
    "    def train(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for i in range(0, len(self.tokens) * 9 // 10 - self.batch_size,\n",
    "                       self.stride):\n",
    "            self.features.append(self.tokens[i: i + self.batch_size])\n",
    "            self.labels.append(self.tokens[i + 1: i + self.batch_size + 1])\n",
    "\n",
    "    def eval(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        for i in range(len(self.tokens) * 9 // 10 - self.batch_size + 1,\n",
    "                       len(self.tokens) - self.batch_size,\n",
    "                       self.stride):\n",
    "            self.features.append(self.tokens[i: i + self.batch_size])\n",
    "            self.labels.append(self.tokens[i + 1: i + self.batch_size + 1])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.features[index], self.labels[index]\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.split_text(text)\n",
    "        words = [word if word in self.word2index else '<|unk|>' for word in words]\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = \" \".join([self.index2word[index] for index in tokens])\n",
    "        return re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "\n",
    "    def embedding(self, index):\n",
    "        ebd = np.zeros(len(self.vocabulary))\n",
    "        ebd[index] = 1\n",
    "        return ebd"
   ],
   "id": "51b559d7af500222",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:14.977060Z",
     "start_time": "2025-09-07T20:43:14.965727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ],
   "id": "cddf04158f01892d",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:14.996713Z",
     "start_time": "2025-09-07T20:43:14.989338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(out_size, in_size) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad = p.grad.T @ x.data\n",
    "            self.bias.grad = np.sum(p.grad, axis=0)\n",
    "            x.grad = p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight, self.bias, x}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ],
   "id": "3664383d80f75040",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.046129Z",
     "start_time": "2025-09-07T20:43:15.040777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size, axis=None):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.axis = axis\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(embedding_size, vocabulary_size) / vocabulary_size)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        weights = self.weight.data.T[x.data]\n",
    "        p = Tensor(np.sum(weights, axis=self.axis) if self.axis is not None else weights)\n",
    "\n",
    "        def gradient_fn():\n",
    "            if self.weight.grad is None:\n",
    "                self.weight.grad = np.zeros_like(self.weight.data)\n",
    "            self.weight.grad.T[x.data] += p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]"
   ],
   "id": "c8e545a4ee3232b5",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.095951Z",
     "start_time": "2025-09-07T20:43:15.091665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Triu(Layer):\n",
    "\n",
    "    def __init__(self, value=-np.inf):\n",
    "        super().__init__()\n",
    "        self.value = value\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        if not self.training:\n",
    "            return x\n",
    "\n",
    "        axes = list(range(x.data.ndim))\n",
    "        axes[-2], axes[-1] = axes[-1], axes[-2]\n",
    "        mask = np.triu(np.ones(x.data.shape)).transpose(axes)\n",
    "        p = Tensor(x.data)\n",
    "        p.data[mask == 0] = self.value\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * mask\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "30f56767834e58f4",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.146030Z",
     "start_time": "2025-09-07T20:43:15.140623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Softmax(Layer):\n",
    "\n",
    "    def __init__(self, axis=-1):\n",
    "        super().__init__()\n",
    "        self.axis = axis\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        exp = np.exp(x.data - np.max(x.data, axis=self.axis, keepdims=True))\n",
    "        p = Tensor(exp / np.sum(exp, axis=self.axis, keepdims=True))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = np.zeros_like(x.data)\n",
    "\n",
    "            shape = x.data.shape\n",
    "            axis = self.axis if self.axis >= 0 else len(shape) + self.axis\n",
    "            shapes = list(shape)\n",
    "            shapes.pop(axis)\n",
    "\n",
    "            for idx in np.ndindex(tuple(shapes)):\n",
    "                indices = list(idx)\n",
    "                indices.insert(axis, slice(None))\n",
    "                indices = tuple(indices)\n",
    "\n",
    "                probs = p.data[indices]\n",
    "                grad = p.grad[indices]\n",
    "\n",
    "                probs_col = probs.reshape(-1, 1)\n",
    "                jacobian = np.diagflat(probs) - probs_col @ probs_col.T\n",
    "                x.grad[indices] = jacobian @ grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "8f6c0de3d90253d5",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.197705Z",
     "start_time": "2025-09-07T20:43:15.192817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CELoss:\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        exp = np.exp(p.data - np.max(p.data, axis=-1, keepdims=True))\n",
    "        softmax = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "        log = np.log(softmax + 1e-10)\n",
    "        ce = Tensor(0 - np.sum(y.data * log) / len(p.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad = (softmax - y.data) / len(p.data)\n",
    "\n",
    "        ce.gradient_fn = gradient_fn\n",
    "        ce.parents = {p}\n",
    "        return ce"
   ],
   "id": "7ccbfc2e57a093e5",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.249822Z",
     "start_time": "2025-09-07T20:43:15.243227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = [None for _ in range(len(params))]\n",
    "        self.v = [None for _ in range(len(params))]\n",
    "        self.t = 0\n",
    "\n",
    "    def backward(self):\n",
    "        self.t += 1\n",
    "        for idx, p in enumerate(self.parameters):\n",
    "            if p is not None and p.grad is not None:\n",
    "                grad = p.grad.reshape(p.data.shape)\n",
    "\n",
    "                if self.m[idx] is None:\n",
    "                    self.m[idx] = np.zeros_like(p.data)\n",
    "                    self.v[idx] = np.zeros_like(p.data)\n",
    "\n",
    "                self.m[idx] = self.beta1 * self.m[idx] + (1 - self.beta1) * grad\n",
    "                self.v[idx] = self.beta2 * self.v[idx] + (1 - self.beta2) * (grad ** 2)\n",
    "                m_hat = self.m[idx] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[idx] / (1 - self.beta2 ** self.t)\n",
    "                p.data -= m_hat / (np.sqrt(v_hat) + self.eps) * self.lr"
   ],
   "id": "e45f027523b48d14",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.301793Z",
     "start_time": "2025-09-07T20:43:15.295214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTEmbedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, context_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.token_embedding = Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.positional_embedding = Embedding(self.context_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.token_embedding,\n",
    "                       self.positional_embedding]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        token = self.token_embedding(x)\n",
    "        position = self.positional_embedding(Tensor(range(self.context_size)))\n",
    "        return token + position"
   ],
   "id": "3f01131b79245722",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.354696Z",
     "start_time": "2025-09-07T20:43:15.346786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTAttention(Layer):\n",
    "\n",
    "    def __init__(self, context_size, embedding_size, heads=1):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "\n",
    "        self.attention_query = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.attention_key = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.attention_value = Linear(self.embedding_size, self.embedding_size * self.heads)\n",
    "        self.triu = Triu(self.context_size)\n",
    "        self.softmax = Softmax()\n",
    "        self.merge = Linear(self.heads * self.embedding_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.attention_query,\n",
    "                       self.attention_key,\n",
    "                       self.attention_value,\n",
    "                       self.triu,\n",
    "                       self.softmax,\n",
    "                       self.merge]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        query = self.attention_query(x).reshape((-1, self.heads, self.embedding_size))\n",
    "        key = self.attention_key(x).reshape((-1, self.heads, self.embedding_size))\n",
    "        value = self.attention_value(x).reshape((-1, self.heads, self.embedding_size))\n",
    "\n",
    "        scores = self.triu(query @ key.transpose((0, 2, 1)))\n",
    "        weights = self.softmax(scores)\n",
    "        vectors = self.merge((weights @ value).reshape((-1, self.heads * self.embedding_size)))\n",
    "        return vectors"
   ],
   "id": "cf618dae3907c590",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.403639Z",
     "start_time": "2025-09-07T20:43:15.400222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPTOutput(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.output = Linear(self.embedding_size, self.vocabulary_size)\n",
    "\n",
    "        self.layers = [self.output]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        return self.output(x)"
   ],
   "id": "4fe6a4e0f82e5115",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.454899Z",
     "start_time": "2025-09-07T20:43:15.449422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, context_size, embedding_size, heads=1):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.heads = heads\n",
    "\n",
    "        self.embedding = GPTEmbedding(self.vocabulary_size, self.context_size, self.embedding_size)\n",
    "        self.attention = GPTAttention(self.context_size, self.embedding_size, self.heads)\n",
    "        self.output = GPTOutput(self.vocabulary_size, self.embedding_size)\n",
    "\n",
    "        self.layers = [self.embedding,\n",
    "                       self.attention,\n",
    "                       self.output]\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        x = self.embedding(x)\n",
    "        x = self.attention(x)\n",
    "        return self.output(x)"
   ],
   "id": "36457a9162433b7b",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:15.504917Z",
     "start_time": "2025-09-07T20:43:15.501772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CONTEXT_SIZE = 4\n",
    "EMBEDDING_SIZE = 3\n",
    "HEADS = 2\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10"
   ],
   "id": "7222b6308f2e8c38",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:20.222646Z",
     "start_time": "2025-09-07T20:43:15.554507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DataLoader('../one-day.txt', CONTEXT_SIZE, 1)\n",
    "\n",
    "model = GPT(len(dataset.vocabulary), CONTEXT_SIZE, EMBEDDING_SIZE, HEADS)\n",
    "loss = CELoss()\n",
    "sgd = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(len(dataset)):\n",
    "        feature, label = dataset[i]\n",
    "\n",
    "        prediction = model(Tensor(feature))\n",
    "        error = loss(prediction, Tensor(dataset.embedding(label)))\n",
    "        error.gradient()\n",
    "        sgd.backward()\n",
    "\n",
    "print(f'Prediction: {prediction.data}')\n",
    "print(f'Error: {error.data}')"
   ],
   "id": "e36cc82254f6134c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[0.02541429 0.03246999 0.01097701 ... 0.01045085 0.0218466  0.02200184]\n",
      " [0.02038427 0.02618106 0.00878813 ... 0.00857598 0.01796318 0.01748376]\n",
      " [0.03258448 0.04169134 0.01406592 ... 0.01348659 0.02820715 0.02813524]\n",
      " [0.01671638 0.0214421  0.00720711 ... 0.00700047 0.01465201 0.01436377]]\n",
      "Error: 22.367056605116304\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T20:43:20.305407Z",
     "start_time": "2025-09-07T20:43:20.270142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset.eval()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    feature, label = dataset[i]\n",
    "\n",
    "    print(\"Feature: \", dataset.decode(feature))\n",
    "    print(\"Label: \", dataset.decode(label))\n",
    "\n",
    "    prediction = model(Tensor(feature))\n",
    "    prediction_tokens = []\n",
    "    for j in range(len(label)):\n",
    "        prediction_tokens.append(prediction.data[j].argmax())\n",
    "    print(\"Prediction: \", dataset.decode(prediction_tokens))"
   ],
   "id": "a26cce3b3764a229",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature:  about his day.\n",
      "Label:  his day. he\n",
      "Prediction:  into into into into\n",
      "Feature:  his day. he\n",
      "Label:  day. he shows\n",
      "Prediction:  into into into into\n",
      "Feature:  day. he shows\n",
      "Label:  . he shows his\n",
      "Prediction:  into into into into\n",
      "Feature:  . he shows his\n",
      "Label:  he shows his notebook\n",
      "Prediction:  into into into into\n",
      "Feature:  he shows his notebook\n",
      "Label:  shows his notebook.\n",
      "Prediction:  into into into into\n",
      "Feature:  shows his notebook.\n",
      "Label:  his notebook.\"\n",
      "Prediction:  into into into into\n",
      "Feature:  his notebook.\"\n",
      "Label:  notebook.\" you\n",
      "Prediction:  into into into into\n",
      "Feature:  notebook.\" you\n",
      "Label:  .\" you had\n",
      "Prediction:  into into into into\n",
      "Feature:  .\" you had\n",
      "Label:  \" you had quite\n",
      "Prediction:  into into into into\n",
      "Feature:  \" you had quite\n",
      "Label:  you had quite an\n",
      "Prediction:  into into into into\n",
      "Feature:  you had quite an\n",
      "Label:  had quite an adventure\n",
      "Prediction:  into into into into\n",
      "Feature:  had quite an adventure\n",
      "Label:  quite an adventure,\n",
      "Prediction:  into into into into\n",
      "Feature:  quite an adventure,\n",
      "Label:  an adventure,\"\n",
      "Prediction:  into into into into\n",
      "Feature:  an adventure,\"\n",
      "Label:  adventure,\" his\n",
      "Prediction:  into into into into\n",
      "Feature:  adventure,\" his\n",
      "Label:  ,\" his father\n",
      "Prediction:  into into into into\n",
      "Feature:  ,\" his father\n",
      "Label:  \" his father says\n",
      "Prediction:  into into into into\n",
      "Feature:  \" his father says\n",
      "Label:  his father says.\n",
      "Prediction:  into into into into\n",
      "Feature:  his father says.\n",
      "Label:  father says.\"\n",
      "Prediction:  into into into into\n",
      "Feature:  father says.\"\n",
      "Label:  says.\" yes\n",
      "Prediction:  into into into into\n",
      "Feature:  says.\" yes\n",
      "Label:  .\" yes,\n",
      "Prediction:  into into into into\n",
      "Feature:  .\" yes,\n",
      "Label:  \" yes,\"\n",
      "Prediction:  into into into into\n",
      "Feature:  \" yes,\"\n",
      "Label:  yes,\" tom\n",
      "Prediction:  into into into into\n",
      "Feature:  yes,\" tom\n",
      "Label:  ,\" tom smiles\n",
      "Prediction:  into into into into\n",
      "Feature:  ,\" tom smiles\n",
      "Label:  \" tom smiles.\n",
      "Prediction:  into into into into\n",
      "Feature:  \" tom smiles.\n",
      "Label:  tom smiles. night\n",
      "Prediction:  into into into into\n",
      "Feature:  tom smiles. night\n",
      "Label:  smiles. night falls\n",
      "Prediction:  into into into into\n",
      "Feature:  smiles. night falls\n",
      "Label:  . night falls after\n",
      "Prediction:  into into into into\n",
      "Feature:  . night falls after\n",
      "Label:  night falls after dinner\n",
      "Prediction:  into into into into\n",
      "Feature:  night falls after dinner\n",
      "Label:  falls after dinner,\n",
      "Prediction:  into into into into\n",
      "Feature:  falls after dinner,\n",
      "Label:  after dinner, tom\n",
      "Prediction:  into into into into\n",
      "Feature:  after dinner, tom\n",
      "Label:  dinner, tom takes\n",
      "Prediction:  into into into into\n",
      "Feature:  dinner, tom takes\n",
      "Label:  , tom takes a\n",
      "Prediction:  into into into into\n",
      "Feature:  , tom takes a\n",
      "Label:  tom takes a bath\n",
      "Prediction:  into into into into\n",
      "Feature:  tom takes a bath\n",
      "Label:  takes a bath.\n",
      "Prediction:  into into into into\n",
      "Feature:  takes a bath.\n",
      "Label:  a bath. he\n",
      "Prediction:  into into into into\n",
      "Feature:  a bath. he\n",
      "Label:  bath. he puts\n",
      "Prediction:  into into into into\n",
      "Feature:  bath. he puts\n",
      "Label:  . he puts on\n",
      "Prediction:  into into into into\n",
      "Feature:  . he puts on\n",
      "Label:  he puts on his\n",
      "Prediction:  into into into into\n",
      "Feature:  he puts on his\n",
      "Label:  puts on his pajamas\n",
      "Prediction:  into into into into\n",
      "Feature:  puts on his pajamas\n",
      "Label:  on his pajamas.\n",
      "Prediction:  into into into into\n",
      "Feature:  on his pajamas.\n",
      "Label:  his pajamas. he\n",
      "Prediction:  into into into into\n",
      "Feature:  his pajamas. he\n",
      "Label:  pajamas. he brushes\n",
      "Prediction:  into into into into\n",
      "Feature:  pajamas. he brushes\n",
      "Label:  . he brushes his\n",
      "Prediction:  into into into into\n",
      "Feature:  . he brushes his\n",
      "Label:  he brushes his teeth\n",
      "Prediction:  into into into into\n",
      "Feature:  he brushes his teeth\n",
      "Label:  brushes his teeth.\n",
      "Prediction:  into into into into\n",
      "Feature:  brushes his teeth.\n",
      "Label:  his teeth. he\n",
      "Prediction:  into into into into\n",
      "Feature:  his teeth. he\n",
      "Label:  teeth. he lies\n",
      "Prediction:  into into into into\n",
      "Feature:  teeth. he lies\n",
      "Label:  . he lies in\n",
      "Prediction:  into into into into\n",
      "Feature:  . he lies in\n",
      "Label:  he lies in bed\n",
      "Prediction:  into into into into\n",
      "Feature:  he lies in bed\n",
      "Label:  lies in bed and\n",
      "Prediction:  into into into into\n",
      "Feature:  lies in bed and\n",
      "Label:  in bed and looks\n",
      "Prediction:  into into into into\n",
      "Feature:  in bed and looks\n",
      "Label:  bed and looks at\n",
      "Prediction:  into into into into\n",
      "Feature:  bed and looks at\n",
      "Label:  and looks at the\n",
      "Prediction:  into into into into\n",
      "Feature:  and looks at the\n",
      "Label:  looks at the ceiling\n",
      "Prediction:  into into into into\n",
      "Feature:  looks at the ceiling\n",
      "Label:  at the ceiling.\n",
      "Prediction:  into into into into\n",
      "Feature:  at the ceiling.\n",
      "Label:  the ceiling. he\n",
      "Prediction:  into into into into\n",
      "Feature:  the ceiling. he\n",
      "Label:  ceiling. he thinks\n",
      "Prediction:  into into into into\n",
      "Feature:  ceiling. he thinks\n",
      "Label:  . he thinks about\n",
      "Prediction:  into into into into\n",
      "Feature:  . he thinks about\n",
      "Label:  he thinks about his\n",
      "Prediction:  into into into into\n",
      "Feature:  he thinks about his\n",
      "Label:  thinks about his day\n",
      "Prediction:  into into into into\n",
      "Feature:  thinks about his day\n",
      "Label:  about his day.\n",
      "Prediction:  into into into into\n",
      "Feature:  about his day.\n",
      "Label:  his day. he\n",
      "Prediction:  into into into into\n",
      "Feature:  his day. he\n",
      "Label:  day. he whispers\n",
      "Prediction:  into into into into\n",
      "Feature:  day. he whispers\n",
      "Label:  . he whispers,\n",
      "Prediction:  into into into into\n",
      "Feature:  . he whispers,\n",
      "Label:  he whispers,\"\n",
      "Prediction:  into into into into\n",
      "Feature:  he whispers,\"\n",
      "Label:  whispers,\" thank\n",
      "Prediction:  into into into into\n",
      "Feature:  whispers,\" thank\n",
      "Label:  ,\" thank you\n",
      "Prediction:  into into into into\n",
      "Feature:  ,\" thank you\n",
      "Label:  \" thank you for\n",
      "Prediction:  into into into into\n",
      "Feature:  \" thank you for\n",
      "Label:  thank you for the\n",
      "Prediction:  into into into into\n",
      "Feature:  thank you for the\n",
      "Label:  you for the adventure\n",
      "Prediction:  into into into into\n",
      "Feature:  you for the adventure\n",
      "Label:  for the adventure.\n",
      "Prediction:  into into into into\n",
      "Feature:  for the adventure.\n",
      "Label:  the adventure.\"\n",
      "Prediction:  into into into into\n",
      "Feature:  the adventure.\"\n",
      "Label:  adventure.\" he\n",
      "Prediction:  into into into into\n",
      "Feature:  adventure.\" he\n",
      "Label:  .\" he closes\n",
      "Prediction:  into into into into\n",
      "Feature:  .\" he closes\n",
      "Label:  \" he closes his\n",
      "Prediction:  into into into into\n",
      "Feature:  \" he closes his\n",
      "Label:  he closes his eyes\n",
      "Prediction:  into into into into\n",
      "Feature:  he closes his eyes\n",
      "Label:  closes his eyes and\n",
      "Prediction:  into into into into\n",
      "Feature:  closes his eyes and\n",
      "Label:  his eyes and falls\n",
      "Prediction:  into into into into\n",
      "Feature:  his eyes and falls\n",
      "Label:  eyes and falls asleep\n",
      "Prediction:  into into into into\n",
      "Feature:  eyes and falls asleep\n",
      "Label:  and falls asleep.\n",
      "Prediction:  into into into into\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
