# 退而结网：构建深度学习框架（TTNN）

目标：将第一部分中的零散函数重构为模块化的类，模仿现代深度学习框架的结构，为后续实现复杂模型打下基础。

### 万物皆对象：封装与抽象

* 核心概念：从面向过程到面向对象。为什么我们需要框架？（代码复用、逻辑清晰、易于扩展）。
* 框架的基石（Tensor类）：将数据和其相关操作封装在Tensor类中，这是我们框架中数据流动的基本单位。
* 第一个模块（Linear类）：将权重、偏置和前向传播的逻辑封装成一个可重用的Linear模块。解释__init__和__call__的作用。
* 数据供给（DataLoader类）：创建一个简单的数据加载器，为模型提供数据。
* 代码实践：用新创建的类重构线性预测的功能，体会代码结构的变化。

### 衡量标准模块化：封装损失函数

* 核心概念：单一职责原则。将损失计算的逻辑从主流程中分离出来。
* MSELoss类：创建一个MSELoss类，使其像一个可调用的函数，接收预测值和真实值，返回损失。
* 为什么需要封装？：为未来支持多种不同的损失函数（如交叉熵损失）打下基础，只需增加新的类，而无需修改训练代码。
* 代码实践：将第一部分中的mse_loss函数改写为MSELoss类，并更新代码。

### 代码的灵魂：自动求导与计算图

* 核心概念：自动求导（Autograd）的魔法。引入动态计算图的思想。
* 升级Tensor：为Tensor增加grad（梯度）、parents（记录来源）和gradient_fn（记录如何计算梯度）三个关键属性。
* 反向传播的自动化：实现Tensor的gradient方法，通过递归调用父节点的梯度计算函数，实现链式法则的自动化。
* 优化器登场（SGD类）：将参数更新的逻辑封装到SGD类中，它负责持有模型参数并根据梯度更新它们。
* 代码实践：重写Linear和MSELoss，让它们在前向传播时构建计算图。然后用 error.gradient() 启动整个反向传播过程。

### 参数的管理者：优化器与学习率

* 核心概念：将模型（如何计算）与优化器（如何更新）分离。
* SGD的职责：明确优化器的角色就是根据参数的grad属性来更新其data属性。
* 超参数的归属：将学习率lr作为SGD类的初始化参数。这使得更换优化算法（如Adam）或调整学习率变得非常简单。
* 代码实践：为SGD类增加lr参数，并在其backward方法中应用它。

### 让数据流动起来：可迭代的数据加载器

* 核心概念：让数据加载更通用和优雅。
* Python的魔法方法：为DataLoader实现__len__和__getitem__方法，使其表现得像一个标准的 Python 序列（如列表）。
* 解耦训练循环：通过这种方式，我们的训练循环可以通过标准的for i in range(len(dataset)) 来迭代，而无需关心 DataLoader 内部是如何存储和管理数据的。
* 代码实践：改造DataLoader并简化训练循环的写法。

### 效率革命：支持批处理

* 核心概念：从处理单个样本（向量）到处理一批样本（矩阵）。这是发挥硬件性能的关键。
* 数据维度的变化：讲解输入、输出和梯度的维度在批处理模式下的变化。
* 框架的适应性改造： 
  * DataLoader：修改__getitem__以支持按批次返回数据。
  * Linear和MSELoss：确保前向和反向传播的数学运算能正确处理矩阵运算。
  * 梯度平均：在反向传播中，梯度需要除以批量大小batch_size来进行平均。
* 代码实践：全面更新框架以适应批处理，这是迈向现代深度学习框架的重要一步。

### 训练的完整周期：引入 Epoch

* 核心概念：将训练循环封装为最终形态。
* Epoch循环：在批处理循环之外，增加一个外层的Epoch循环，代表模型对整个数据集的完整学习轮次。
* 框架的成熟形态：至此，我们的代码已经拥有了一个标准深度学习框架的完整训练流程：Epoch -> Batch -> Forward -> Loss -> Backward -> Step。
* 代码实践：构建双层嵌套循环，完成最终的训练逻辑。

### 像搭乐高一样构建网络：Layer 与 Sequential

* 核心概念：抽象的力量。引入Layer作为所有网络层的基类，并创建Sequential容器来堆叠它们。
* Layer抽象基类：定义所有层都必须遵守的接口规范（如forward和parameters方法）。
* Sequential容器：一个特殊的Layer，它的forward方法会按顺序调用内部所有子Layer的forward方法。它的parameters方法会自动收集所有子Layer的参数。
* 代码实践：
  * 创建Layer和Sequential类。
  * 让Linear继承自Layer。
  * 使用model = Sequential([...])的方式轻松构建多层网络。

### 框架的可扩展性：添加激活函数层

* 核心概念：检验我们框架设计的成功与否。添加新功能是否简单？
* 万物皆Layer：将ReLU激活函数也实现为一个Layer。它没有可训练参数，只有一个forward方法用于计算，以及相应的逻辑来传递梯度。
* 即插即用：展示如何像插入一个Linear层一样，轻松地在Sequential容器中插入ReLU层。
* 终极目标达成：我们成功构建了一个迷你但功能完备、结构清晰且易于扩展的深度学习框架，重现了第一部分的所有功能。
* 代码实践：创建ReLU类，并将其插入到Sequential模型中，完成最终的ANN框架。
