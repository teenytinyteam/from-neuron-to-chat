# 人工神经元网络（ANN）

目标：通过九个步骤，从一个单一的线性单元演进到一个可以训练的、包含隐藏层和激活函数的多层神经网络。

### 最简单的人工神经元网络：线性预测

* 核心概念：什么是人工智能、机器学习和神经网络？
* 从生物学到数学：神经元的简化模型（输入、权重、偏置）。
* 数学表达：讲解线性方程 Y = W * X + b 如何模拟神经元的计算过程。
* 代码实践：实现一个 forward 函数，用给定的权重和偏置进行第一次预测。解释输入特征、权重矩阵和偏置的维度。

### 错得有多离谱？：损失函数

* 核心概念：如何衡量预测的好坏？引入“损失”（Loss）或“成本”（Cost）的概念。
* 均方误差 (MSE)：为什么它是衡量回归问题错误的常用方法？直观解释其“惩罚”大幅度错误的能力。
* 数学表达：讲解 Loss = mean((prediction - label)^2) 的计算过程。
* 代码实践：实现 mse_loss 函数，计算预测值与真实标签之间的差距。

### 机器如何学习？：梯度下降

* 核心概念：学习的本质是“减少损失”。引入梯度下降（Gradient Descent）的直观思想——“下山”。
* 导数与梯度：简单回顾导数的概念，并将其扩展到多维度的梯度。梯度是函数变化最快的方向。
* 反向传播 (Backpropagation)：这是神经网络的灵魂。详细推导损失函数对权重 w 和偏置 b 的偏导数。
* 代码实践：
  * 实现 gradient 函数计算损失的梯度。
  * 实现 backward 函数，根据梯度更新权重和偏置。

### 学习的节奏：学习率

* 核心概念：引入超参数（Hyperparameter）的概念，学习率（Learning Rate）是第一个也是最重要的超参数。
* 直观理解：学习率如何控制“下山”的每一步大小。
* 学习率太大或太小会怎样？：通过图示解释过大导致震荡不收敛，过小导致学习缓慢。
* 代码实践：在 backward 函数中加入 learning_rate，控制参数更新的幅度。

### 从数据中学习：训练迭代

* 核心概念：模型如何从整个数据集中学习？
* 训练循环 (Training Loop)：建立第一个真正的训练循环，遍历数据集中的每一个样本。
* 随机梯度下降 (SGD)：讲解逐样本更新参数的方式。
* 代码实践：使用 for 循环遍历所有数据，在循环中执行“预测 -> 计算损失 -> 反向传播更新”的完整流程。

### 更高效的学习：批处理

* 核心概念：逐样本更新（SGD）的缺点（计算效率低、梯度不稳定）。
* 小批量梯度下降 (Mini-batch GD)：引入“批”（Batch）的概念，一次处理一小批数据。
* 优点：为什么 Mini-batch 是现代深度学习训练的标配？（硬件友好、梯度更稳定）。
* 代码实践：修改训练循环和 backward 函数，使其能处理一批数据（一个矩阵）而不是单个样本（一个向量）。

### 温故而知新：轮次

* 核心概念：为什么只看一遍所有数据是不够的？引入“轮次”（Epoch）的概念。
* Epoch, Batch, Iteration 的关系：清晰地解释这三个核心术语的区别与联系。
* 完整的训练流程：构建包含 Epoch 和 Batch 的双层嵌套训练循环。
* 代码实践：在代码中加入外层的 epoch 循环，让模型可以完整地重复学习整个数据集。

### 增加思考深度：隐藏层

* 核心概念：单个神经元（或一层神经元）的局限性——只能学习线性关系。
* 多层感知机 (MLP)：引入“隐藏层”（Hidden Layer），构建一个真正的“网络”。
* 链式法则：讲解反向传播如何通过链式法则将梯度从输出层传播到隐藏层。
* 代码实践：
  * 定义两套权重和偏置（输入到隐藏层，隐藏层到输出层）。
  * 修改 forward 和 backward 过程，以支持多层结构。

### 打破线性思维：激活函数

* 核心概念：为什么仅仅堆叠线性层是无用的？（因为其等价于一个线性层）。
* 非线性激活：引入激活函数的概念，它是赋予神经网络学习复杂模式能力的关键。
* ReLU (Rectified Linear Unit)：介绍简单而强大的 ReLU 函数及其导数。
* 代码实践：
  * 实现 relu 和 relu_backward 函数。
  * 在 forward 和 backward 过程中，于隐藏层之后加入 ReLU 激活。