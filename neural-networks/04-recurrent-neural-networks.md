# 理解序列：循环神经元网络（RNN & LSTM）

目标：扩展我们的框架以处理文本等序列数据，亲手构建并理解RNN和LSTM的内部工作原理，并将其应用于情感分析和文本生成任务。

### 让机器读懂文字：文本表示

* 核心概念：序列数据的挑战。计算机如何理解“单词”？
* 数据预处理：加载情感分类数据集（如IMDB评论），进行文本清洗（移除标点、HTML标签）、构建词汇表（Vocabulary）。
* 独热编码（One-Hot Encoding）：介绍最基础的词表示方法。讲解其原理以及“维度灾难”和“语义鸿沟”两大缺点。
* 代码实践：升级DataLoader以处理文本数据，实现词汇表构建、文本到索引的转换（encode）和索引到文本的还原（decode）。

### 更聪明的词表示：词嵌入

* 核心概念：词嵌入（Word Embedding）的思想——将高维稀疏的独热编码映射到低维、稠密的向量空间，让语义相近的词在空间中也彼此接近。
* 实现Embedding层：创建一个Embedding层，其本质是一个可学习的查找表（Look-up Table）。
  * 前向传播：根据输入单词的索引，从权重矩阵中“取出”对应的向量。
  * 反向传播：梯度只会更新在输入中出现过的那些单词的向量。
* 代码实践：使用Embedding层替换独热编码，构建一个简单的“词袋模型”来进行情感分类，体会词嵌入带来的巨大进步。

### 拥有短期记忆的神经元：简单循环网络

* 核心概念：引入“时间”维度。RNN的核心思想——神经元的输出不仅取决于当前的输入，还取决于前一时刻的状态（隐藏状态）。
* 循环与共享：通过图示讲解RNN单元如何在一个时间序列上展开，以及权重在不同时间步之间的共享机制。
* 实现RNN单元：这是本部分的核心。
  * 结构：包含Embedding层、用于结合当前输入和上一时刻隐藏状态的Linear层、激活函数以及输出Linear层。
  * 前向传播：在循环中处理序列的每个时间步，不断更新隐藏状态。
* 代码实践：构建并训练一个简单的RNN模型，用于预测文本序列中的下一个单词，初步体验文本生成。

### 分类任务的新武器：交叉熵损失

* 核心概念：回顾为什么MSE不适合分类，并正式引入适用于多分类问题的交叉熵损失（Cross-Entropy Loss）。
* Softmax+Log Loss：讲解交叉熵损失在数学上通常与Softmax函数结合使用，及其直观的物理解释。
* 实现CELoss：为我们的框架添加交叉熵损失层。与BCELoss不同，它直接处理模型的原始输出（logits），内部完成Softmax计算，数值上更稳定。
* 代码实践：切换到CELoss，重新训练我们的文本生成RNN，观察损失下降和生成文本质量的变化。

### 解决长期依赖：长短期记忆网络

* 核心概念：梯度消失与爆炸。为什么简单RNN难以学习长距离的依赖关系？（例如“我在法国长大……我的母语是法语。”）
* LSTM的门控艺术：详细介绍LSTM单元的内部结构——遗忘门（Forget Gate）、输入门（Input Gate）和输出门（Output Gate），以及它们如何像阀门一样控制信息在“细胞状态（Cell State）”这条高速公路上的流动。
* 实现LSTM单元：在RNN的基础上，实现包含三个门和一个细胞更新机制的LSTM层。这是本书最具挑战性但也是最有价值的代码之一。
* 代码实践：用LSTM替换RNN单元，再次进行文本生成任务，直观对比其捕捉长期依赖的能力。

### 更快的收敛：Adam 优化器

* 核心概念：超越SGD，介绍动量（Momentum）和自适应学习率的思想。
* Adam（Adaptive Moment Estimation）：讲解Adam优化器如何结合动量（一阶矩估计）和RMSprop（二阶矩估计）的优点，为每个参数计算自适应的学习率。
* 实现Adam优化器：为我们的框架添加Adam优化器类，包含对一阶矩（m）和二阶矩（v）的追踪和更新逻辑。
* 代码实践：使用Adam优化器重新训练我们的LSTM模型。观察它如何比标准SGD更快、更稳定地收敛，并可能达到更好的性能。这是我们框架的最后一块重要拼图。