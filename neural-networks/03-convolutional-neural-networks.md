# 看见世界：卷积神经元网络（CNN）

目标：利用我们亲手构建的框架，一步步实现用于图像识别的卷积神经网络，并解决一个真实问题。

### 从一维到二维：展平层与新挑战

* 核心概念：为什么全连接网络（ANN）不适合直接处理图像？维度灾难和空间信息丢失。
* 准备第一个真实数据集：介绍MNIST手写数字数据集，并升级DataLoader以加载和归一化图像数据。
* 连接的桥梁（Flatten层）：创建一个Flatten层，将输入的二维图像（矩阵）展平成一维向量，以便连接到后面的全连接层。
* 训练与评估模式，为Layer基类增加train()和eval()方法，为后续引入Dropout等在训练和预测时行为不同的层做准备。
* 代码实践：使用Flatten和Linear层搭建一个简单的模型，在MNIST数据集上进行初步尝试，感受其局限性。

### 对抗过拟合：随机失活

* 核心概念：什么是过拟合（Overfitting）？为什么模型在训练集上表现很好，但在测试集上却一塌糊涂？
* 正则化技术：引入正则化的思想——通过给模型增加一些限制或随机性来防止它“死记硬背”。
* Dropout层：实现一个Dropout层。在训练时，它会以一定概率随机地“丢弃”（置零）一些神经元的输出，迫使网络学习更鲁棒的特征。在评估时，它则保持不变。
* 代码实践：将Dropout层加入到我们的模型中，观察它对训练和测试结果的影响。

### 激活函数的大家族

* 核心概念：ReLU并非唯一的选择。介绍更多常用的激活函数，拓宽我们的“工具箱”。
* Tanh：双曲正切函数，将输出压缩到[-1, 1]之间，通常用于隐藏层。
* Sigmoid：将输出压缩到[0, 1]之间，常用于二分类问题的输出层，表示概率。
* Softmax：Sigmoid 的多分类版本，确保所有输出单元的和为1，从而可以解释为各类别的概率分布。
* 代码实践：为我们的框架实现Tanh、Sigmoid和Softmax层，并尝试在模型中使用它们。

### CNN的“眼睛”：卷积层

* 核心概念：卷积（Convolution）操作的直观理解——像一个移动的“滤镜”，用于提取局部特征（如边缘、角点）。
* 关键参数：讲解卷积核（Kernel）、通道（Channel）、步幅（Stride）和填充（Padding）的概念。
* 实现Convolution2D层：这是本书最核心的章节之一。我们将从零开始，用NumPy实现一个二维卷积层。
  * 前向传播：通过循环和切片，模拟卷积核在输入图像上的滑动计算过程。
  * 反向传播：推导并实现卷积操作的梯度计算，这是最具挑战性的部分。
* 代码实践：将Convolution2D层加入模型，替换掉部分的Linear层，开始构建真正的CNN。

### 去粗取精：池化层

* 核心概念：池化（Pooling），特别是最大池化（Max Pooling）的作用——降低特征图的维度，同时保留最显著的特征，并提供一定程度的平移不变性。
* 实现 Pool2D层：
  * 前向传播：在一个区域内（如2x2）取最大值。
  * 反向传播：梯度只会流向在前向传播中被选为最大值的那个元素，其他位置的梯度为零。
* 代码实践：在卷积层之后加入Pool2D层，观察它如何减小数据尺寸，并为后续的全连接层减负。

### 为分类问题量身定做：交叉熵损失

* 核心概念：为什么均方误差（MSE）不适合分类问题？引入更适合衡量概率分布差异的交叉熵损失（Cross-Entropy Loss）。
* 信息论视角：直观解释交叉熵如何衡量两个概率分布的“距离”。
* 实现BCELoss：实现二元交叉熵损失函数。这对于处理多分类问题（通过“一对多”策略）至关重要。
* 构建完整的CNN：组合Convolution2D, Pool2D, Flatten, Linear, Dropout和激活函数，构建一个经典的LeNet-style CNN模型。
* 代码实践：使用新的BCELoss作为损失函数，对完整的CNN模型进行端到端的训练和评估，见证我们亲手构建的框架解决实际问题的强大能力。