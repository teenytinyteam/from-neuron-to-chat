{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Long Short Term Memory",
   "id": "c86b2a6605b72fcd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.011739Z",
     "start_time": "2025-09-07T03:02:31.941491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(99)"
   ],
   "id": "20c0e177ca7b09dd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.032828Z",
     "start_time": "2025-09-07T03:02:32.019256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = None\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def gradient(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.gradient()\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __add__(self, other):\n",
    "        p = Tensor(self.data + other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        p = Tensor(self.data * other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad * other.data\n",
    "            other.grad = p.grad * self.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def concat(self, other, axis):\n",
    "        p = Tensor(np.concatenate([self.data, other.data], axis=axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad, other.grad = np.split(p.grad, [self.data.shape[axis]], axis=axis)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n"
   ],
   "id": "720cee94a4bd1d8f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.078398Z",
     "start_time": "2025-09-07T03:02:32.074161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequence:\n",
    "\n",
    "    def __init__(self, tokens, vocabulary_size, batch_size):\n",
    "        self.tokens = tokens\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):  # 3\n",
    "        return len(self.tokens) - self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):  # 4\n",
    "        return (Tensor([self.tokens[index:index + self.batch_size]]),\n",
    "                Tensor([self.embedding(self.tokens[index + self.batch_size:\n",
    "                                                   index + self.batch_size + 1])]))\n",
    "\n",
    "    def embedding(self, index):\n",
    "        ebd = np.zeros(self.vocabulary_size)\n",
    "        ebd[index] = 1\n",
    "        return ebd"
   ],
   "id": "68715fc46852f113",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.132293Z",
     "start_time": "2025-09-07T03:02:32.123658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, sequence_batch_size):\n",
    "        self.sequence_batch_size = sequence_batch_size\n",
    "\n",
    "        self.reviews = []\n",
    "        self.sentiments = []\n",
    "        with open('reviews.csv', 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for _, row in enumerate(reader):\n",
    "                self.reviews.append(row[0])\n",
    "                self.sentiments.append(row[1])\n",
    "\n",
    "        split_reviews = []\n",
    "        for r in self.reviews:\n",
    "            split_reviews.append(self.clean_text(r.lower()).split())\n",
    "\n",
    "        self.vocabulary = set(w for r in split_reviews for w in r)\n",
    "        self.word2index = {w: idx for idx, w in enumerate(self.vocabulary)}\n",
    "        self.index2word = {idx: w for idx, w in enumerate(self.vocabulary)}\n",
    "        self.tokens = [[self.word2index[w] for w in r if w in self.word2index] for r in split_reviews]\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        txt = re.sub(r'<[^>]+>', '', text)\n",
    "        txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.clean_text(text.lower()).split()\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([self.index2word[index] for index in tokens])\n",
    "\n",
    "    def train(self):\n",
    "        self.sequences = self.tokens[:-10]\n",
    "\n",
    "    def eval(self):\n",
    "        self.sequences = self.tokens[-10:]\n",
    "\n",
    "    def __len__(self):  # 3\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):  # 4\n",
    "        return Sequence(self.sequences[index], len(self.vocabulary), self.sequence_batch_size)"
   ],
   "id": "64d039563fea9e4d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.182562Z",
     "start_time": "2025-09-07T03:02:32.176978Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ],
   "id": "ab0c14d433a51fd4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.237575Z",
     "start_time": "2025-09-07T03:02:32.225896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(out_size, in_size) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad = p.grad.T @ x.data\n",
    "            self.bias.grad = np.sum(p.grad, axis=0)\n",
    "            x.grad = p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight, self.bias, x}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ],
   "id": "239e25108f85cdaf",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.285058Z",
     "start_time": "2025-09-07T03:02:32.278015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "\n",
    "    def train(self):\n",
    "        for l in self.layers:\n",
    "            l.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for l in self.layers:\n",
    "            l.eval()"
   ],
   "id": "4597c09c6b124252",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.337960Z",
     "start_time": "2025-09-07T03:02:32.329946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size, axis=1):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.axis = axis\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(embedding_size, vocabulary_size) / vocabulary_size)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.sum(self.weight.data.T[x.data], axis=self.axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            if self.weight.grad is None:\n",
    "                self.weight.grad = np.zeros_like(self.weight.data)\n",
    "            self.weight.grad.T[x.data] += p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]"
   ],
   "id": "6cd3378f705732a9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.389479Z",
     "start_time": "2025-09-07T03:02:32.383959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tanh(Layer):\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.tanh(x.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * (1 - p.data ** 2)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "e147fdb91dc70998",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.443425Z",
     "start_time": "2025-09-07T03:02:32.435835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "    def __init__(self, clip_range=(-100, 100)):\n",
    "        super().__init__()\n",
    "        self.clip_range = clip_range\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        z = np.clip(x.data, self.clip_range[0], self.clip_range[1])\n",
    "        p = Tensor(1 / (1 + np.exp(-z)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * p.data * (1 - p.data)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "8241282a0fa71aee",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.490424Z",
     "start_time": "2025-09-07T03:02:32.486451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CELoss:\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        exp = np.exp(p.data - np.max(p.data, axis=-1, keepdims=True))\n",
    "        softmax = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "        log = np.log(softmax + 1e-10)\n",
    "        ce = Tensor(0 - np.sum(y.data * log) / len(p.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad = (softmax - y.data) / len(p.data)\n",
    "\n",
    "        ce.gradient_fn = gradient_fn\n",
    "        ce.parents = {p}\n",
    "        return ce"
   ],
   "id": "c96eab2b2718fba9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.541816Z",
     "start_time": "2025-09-07T03:02:32.536169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SGD:\n",
    "\n",
    "    def __init__(self, parameters, lr):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "\n",
    "    def backward(self):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad * self.lr"
   ],
   "id": "83adc1094c04eabf",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.600562Z",
     "start_time": "2025-09-07T03:02:32.587971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTM(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = Embedding(vocabulary_size, embedding_size)\n",
    "        self.forget_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.input_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.output_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.cell_update = Linear(embedding_size * 2, embedding_size)\n",
    "        self.output = Linear(embedding_size, vocabulary_size)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "        self.layers = [self.embedding,\n",
    "                       self.forget_gate,\n",
    "                       self.input_gate,\n",
    "                       self.output_gate,\n",
    "                       self.cell_update,\n",
    "                       self.output,\n",
    "                       self.sigmoid,\n",
    "                       self.tanh]\n",
    "\n",
    "    def __call__(self, x: Tensor, c: Tensor, h: Tensor):\n",
    "        return self.forward(x, c, h)\n",
    "\n",
    "    def forward(self, x: Tensor, c: Tensor, h: Tensor):\n",
    "        if not c:\n",
    "            c = Tensor(np.zeros((1, self.embedding_size)))\n",
    "        if not h:\n",
    "            h = Tensor(np.zeros((1, self.embedding_size)))\n",
    "\n",
    "        embedding_feature = self.embedding(x)\n",
    "        concat_feature = embedding_feature.concat(h, axis=1)\n",
    "        forget_hidden = self.sigmoid(self.forget_gate(concat_feature))\n",
    "        input_hidden = self.sigmoid(self.input_gate(concat_feature))\n",
    "        output_hidden = self.sigmoid(self.output_gate(concat_feature))\n",
    "        cell_hidden = self.tanh(self.cell_update(concat_feature))\n",
    "        cell_feature = forget_hidden * c + input_hidden * cell_hidden\n",
    "        hidden_feature = output_hidden * self.tanh(cell_feature)\n",
    "\n",
    "        return (self.output(hidden_feature),\n",
    "                Tensor(cell_feature.data),\n",
    "                Tensor(hidden_feature.data))\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]"
   ],
   "id": "27fecc9f35db2b40",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:32.651404Z",
     "start_time": "2025-09-07T03:02:32.646883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 0.02\n",
    "BATCHES = 2\n",
    "EPOCHS = 10"
   ],
   "id": "70114198993ae07a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:40.964516Z",
     "start_time": "2025-09-07T03:02:32.697939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DataLoader(BATCHES)\n",
    "\n",
    "model = LSTM(len(dataset.vocabulary), 64)\n",
    "\n",
    "loss = CELoss()\n",
    "sgd = SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(len(dataset)):\n",
    "        sequence = dataset[i]\n",
    "\n",
    "        cell = hidden = None\n",
    "        for i in range(len(sequence)):\n",
    "            feature, label = sequence[i]\n",
    "\n",
    "            prediction, cell, hidden = model(feature, cell, hidden)\n",
    "            error = loss(prediction, label)\n",
    "\n",
    "            error.gradient()\n",
    "            sgd.backward()\n",
    "\n",
    "print(f'Prediction: {prediction.data}')\n",
    "print(f'Error: {error.data}')"
   ],
   "id": "c91ac064cece17c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[-0.93798411  0.4911417  -0.11154155  0.51804935  0.06640075 -0.93580084\n",
      "  -0.93444315 -0.72409664 -2.35024194 -0.08388275 -0.62186398 -0.06347789\n",
      "  -0.83103121 -0.87093843 -0.67938979  0.44428378 -0.30342118 -0.51982411\n",
      "  -0.76820929  0.50804689 -0.95645536  6.82384488  0.296484    2.57684255\n",
      "  -0.72242256  1.3310878  -0.06362267  1.61691078 -0.64349568 -0.48945141\n",
      "  -0.40067393  1.56438469 -0.26289259 -0.61335493  0.55970473 -0.73275055\n",
      "  -0.21004898 -0.72222561 -0.91419266 -0.37993973 -1.37468615 -0.68916369\n",
      "   0.75806775 -0.97652255 -0.73833584 -1.13049271 -0.8701522   1.95822624\n",
      "   0.32927865  0.41530536  1.34331704 -0.23186723 -0.8236861   1.90097933\n",
      "  -1.05986737 -0.59665934 -0.80641513  1.16610249  0.93550778 -0.07417397\n",
      "   0.56053606 -0.95313499 -0.65142879  0.71224323  1.03372336  3.87205552\n",
      "  -1.09908996 -2.00427331 -0.87998705  1.3758861   0.19029811 -0.68861757\n",
      "  -1.08700392 -0.81801658  0.25517476 -1.51519516  0.75511229 -0.8291522\n",
      "  -0.24689281 -0.10481102  0.54382661 -0.32286436  1.41977773  0.13437436\n",
      "   0.82706362 -0.83711386]]\n",
      "Error: 5.3736302304567936\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:02:41.045712Z",
     "start_time": "2025-09-07T03:02:40.992917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset.eval()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sequence = dataset[i]\n",
    "\n",
    "    feature, label = sequence[0]\n",
    "    original = sequence.tokens[:BATCHES]\n",
    "    generated = original.copy()\n",
    "\n",
    "    cell = hidden = None\n",
    "    for j in range(len(sequence)):\n",
    "        feature, label = sequence[j]\n",
    "\n",
    "        prediction, cell, hidden = model(feature, cell, hidden)\n",
    "        original.append(sequence.tokens[j + BATCHES])\n",
    "        generated.append(prediction.data.argmax())\n",
    "\n",
    "    print(f'original: {dataset.decode(original)}')\n",
    "    print(f'generated: {dataset.decode(generated)}')"
   ],
   "id": "96a2b4ed9ca2e2e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: worst movie with awful music the actor did a boring job actress director character screenplay scene but or it at by\n",
      "generated: worst movie the boring screenplay the screenplay and a poor job actor was screenplay screenplay scene scene in her in by\n",
      "original: i recommend this film the actress was wonderful and the music was amazing actor director character scene plot by was\n",
      "generated: i recommend this screenplay her and and wonderful i the actor was i i screenplay screenplay screenplay screenplay screenplay by\n",
      "original: poor movie the story and actor were terrible i disappointed the director actress character action screenplay scene by is\n",
      "generated: poor movie the actor and the the i i the the actor and screenplay screenplay screenplay scene scene was\n",
      "original: excellent film i saw this time perfect performance by the actor and actress director screenplay scene character his her\n",
      "generated: excellent film i this the time i performance screenplay the actress was were were screenplay screenplay screenplay screenplay her\n",
      "original: boring movie with poor effect the director did a awful job actor character actress screenplay music plot by was is\n",
      "generated: boring movie the boring was the actress was a poor the actor was screenplay screenplay scene in her was i\n",
      "original: this film was wonderful i loved the plot and actress the scene was amazing director character actor screenplay his her\n",
      "generated: this film was i i the the actor and actor were actor was and i screenplay screenplay screenplay scene in\n",
      "original: great movie the actor and screenplay were excellent i enjoyed the visual actress character scene script music effect or\n",
      "generated: great movie the actor and the were i i the the actor and character screenplay screenplay screenplay screenplay screenplay\n",
      "original: i waste of time the screenplay was boring and the scene was awful actor actress character screenplay music effect by\n",
      "generated: i waste job time the actor was awful i the actor was and and was screenplay screenplay scene in at\n",
      "original: best film with fantastic soundtrack the director did a perfect job actor actress character scene screenplay music in on\n",
      "generated: best film job was actress actress was was a poor job was was screenplay screenplay screenplay scene in at\n",
      "original: this movie was terrible i hated the story and actor the scene was poor actress director character screenplay action by\n",
      "generated: this movie was director i the the actor and the the actor was and i was screenplay screenplay scene screenplay\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
