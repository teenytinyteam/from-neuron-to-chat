{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": "# Adaptive Moment Estimation",
   "id": "f914266969fa2560"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:23.863439Z",
     "start_time": "2025-09-07T03:04:23.703587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import re\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np"
   ],
   "id": "beeab03603cf9a04",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:23.880856Z",
     "start_time": "2025-09-07T03:04:23.872206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tensor:\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = np.array(data)\n",
    "        self.grad = None\n",
    "        self.gradient_fn = lambda: None\n",
    "        self.parents = set()\n",
    "\n",
    "    def gradient(self):\n",
    "        if self.gradient_fn:\n",
    "            self.gradient_fn()\n",
    "\n",
    "        for p in self.parents:\n",
    "            p.gradient()\n",
    "\n",
    "    def shape(self):\n",
    "        return self.data.shape\n",
    "\n",
    "    def size(self):\n",
    "        return np.prod(self.data.shape[1:])\n",
    "\n",
    "    def __add__(self, other):\n",
    "        p = Tensor(self.data + other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad\n",
    "            other.grad = p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        p = Tensor(self.data * other.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad = p.grad * other.data\n",
    "            other.grad = p.grad * self.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p\n",
    "\n",
    "    def concat(self, other, axis):\n",
    "        p = Tensor(np.concatenate([self.data, other.data], axis=axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.grad, other.grad = np.split(p.grad, [self.data.shape[axis]], axis=axis)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self, other}\n",
    "        return p"
   ],
   "id": "aab52353ea591328",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:23.927798Z",
     "start_time": "2025-09-07T03:04:23.921920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequence:\n",
    "\n",
    "    def __init__(self, tokens, vocabulary_size, batch_size):\n",
    "        self.tokens = tokens\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):  # 3\n",
    "        return len(self.tokens) - self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):  # 4\n",
    "        return (Tensor([self.tokens[index:index + self.batch_size]]),\n",
    "                Tensor([self.embedding(self.tokens[index + self.batch_size:\n",
    "                                                   index + self.batch_size + 1])]))\n",
    "\n",
    "    def embedding(self, index):\n",
    "        ebd = np.zeros(self.vocabulary_size)\n",
    "        ebd[index] = 1\n",
    "        return ebd"
   ],
   "id": "d8615dd4516bfa91",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:23.983495Z",
     "start_time": "2025-09-07T03:04:23.974190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, sequence_batch_size):\n",
    "        self.sequence_batch_size = sequence_batch_size\n",
    "\n",
    "        self.reviews = []\n",
    "        self.sentiments = []\n",
    "        with open('reviews.csv', 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for _, row in enumerate(reader):\n",
    "                self.reviews.append(row[0])\n",
    "                self.sentiments.append(row[1])\n",
    "\n",
    "        split_reviews = []\n",
    "        for r in self.reviews:\n",
    "            split_reviews.append(self.clean_text(r.lower()).split())\n",
    "\n",
    "        self.vocabulary = set(w for r in split_reviews for w in r)\n",
    "        self.word2index = {w: idx for idx, w in enumerate(self.vocabulary)}\n",
    "        self.index2word = {idx: w for idx, w in enumerate(self.vocabulary)}\n",
    "        self.tokens = [[self.word2index[w] for w in r if w in self.word2index] for r in split_reviews]\n",
    "\n",
    "        self.train()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        txt = re.sub(r'<[^>]+>', '', text)\n",
    "        txt = re.sub(r'[^a-zA-Z0-9\\s]', '', txt)\n",
    "        return txt\n",
    "\n",
    "    def encode(self, text):\n",
    "        words = self.clean_text(text.lower()).split()\n",
    "        return [self.word2index[word] for word in words]\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        return \" \".join([self.index2word[index] for index in tokens])\n",
    "\n",
    "    def train(self):\n",
    "        self.sequences = self.tokens[:-10]\n",
    "\n",
    "    def eval(self):\n",
    "        self.sequences = self.tokens[-10:]\n",
    "\n",
    "    def __len__(self):  # 3\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):  # 4\n",
    "        return Sequence(self.sequences[index], len(self.vocabulary), self.sequence_batch_size)"
   ],
   "id": "df31af8637130c50",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.036511Z",
     "start_time": "2025-09-07T03:04:24.029947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer(ABC):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args):\n",
    "        return self.forward(*args)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args):\n",
    "        pass\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False"
   ],
   "id": "6b6bd081c5b67322",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.105269Z",
     "start_time": "2025-09-07T03:04:24.083892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(99)\n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(out_size, in_size) / in_size)\n",
    "        self.bias = Tensor(np.zeros(out_size))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(x.data @ self.weight.data.T + self.bias.data)\n",
    "\n",
    "        def gradient_fn():\n",
    "            self.weight.grad = p.grad.T @ x.data\n",
    "            self.bias.grad = np.sum(p.grad, axis=0)\n",
    "            x.grad = p.grad @ self.weight.data\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight, self.bias, x}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight, self.bias]"
   ],
   "id": "217b32c7a06228ec",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.148024Z",
     "start_time": "2025-09-07T03:04:24.140815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sequential(Layer):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]\n",
    "\n",
    "    def train(self):\n",
    "        for l in self.layers:\n",
    "            l.train()\n",
    "\n",
    "    def eval(self):\n",
    "        for l in self.layers:\n",
    "            l.eval()"
   ],
   "id": "bfbef23e6bb5036b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.204729Z",
     "start_time": "2025-09-07T03:04:24.196200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size, axis=1):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.axis = axis\n",
    "\n",
    "        self.weight = Tensor(np.random.rand(embedding_size, vocabulary_size) / vocabulary_size)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.sum(self.weight.data.T[x.data], axis=self.axis))\n",
    "\n",
    "        def gradient_fn():\n",
    "            if self.weight.grad is None:\n",
    "                self.weight.grad = np.zeros_like(self.weight.data)\n",
    "            self.weight.grad.T[x.data] += p.grad\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {self.weight}\n",
    "        return p\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]"
   ],
   "id": "d5e8e596330dc9d7",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.256896Z",
     "start_time": "2025-09-07T03:04:24.251068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tanh(Layer):\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        p = Tensor(np.tanh(x.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * (1 - p.data ** 2)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "2513ace340b0a07c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.311611Z",
     "start_time": "2025-09-07T03:04:24.304992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Sigmoid(Layer):\n",
    "\n",
    "    def __init__(self, clip_range=(-100, 100)):\n",
    "        super().__init__()\n",
    "        self.clip_range = clip_range\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        z = np.clip(x.data, self.clip_range[0], self.clip_range[1])\n",
    "        p = Tensor(1 / (1 + np.exp(-z)))\n",
    "\n",
    "        def gradient_fn():\n",
    "            x.grad = p.grad * p.data * (1 - p.data)\n",
    "\n",
    "        p.gradient_fn = gradient_fn\n",
    "        p.parents = {x}\n",
    "        return p"
   ],
   "id": "5e7f0dfef7e57367",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.368553Z",
     "start_time": "2025-09-07T03:04:24.360864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CELoss:\n",
    "\n",
    "    def __call__(self, p: Tensor, y: Tensor):\n",
    "        exp = np.exp(p.data - np.max(p.data, axis=-1, keepdims=True))\n",
    "        softmax = exp / np.sum(exp, axis=-1, keepdims=True)\n",
    "\n",
    "        log = np.log(softmax + 1e-10)\n",
    "        ce = Tensor(0 - np.sum(y.data * log) / len(p.data))\n",
    "\n",
    "        def gradient_fn():\n",
    "            p.grad = (softmax - y.data) / len(p.data)\n",
    "\n",
    "        ce.gradient_fn = gradient_fn\n",
    "        ce.parents = {p}\n",
    "        return ce"
   ],
   "id": "2822ef0df5307dda",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.433337Z",
     "start_time": "2025-09-07T03:04:24.417037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam:\n",
    "\n",
    "    def __init__(self, params, lr=0.01, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.parameters = params\n",
    "        self.lr = lr\n",
    "        self.beta1, self.beta2 = betas\n",
    "        self.eps = eps\n",
    "\n",
    "        self.m = [None for _ in range(len(params))]\n",
    "        self.v = [None for _ in range(len(params))]\n",
    "        self.t = 0\n",
    "\n",
    "    def backward(self):\n",
    "        self.t += 1\n",
    "        for idx, p in enumerate(self.parameters):\n",
    "            if p is not None and p.grad is not None:\n",
    "                grad = p.grad.reshape(p.data.shape)\n",
    "\n",
    "                if self.m[idx] is None:\n",
    "                    self.m[idx] = np.zeros_like(p.data)\n",
    "                    self.v[idx] = np.zeros_like(p.data)\n",
    "\n",
    "                self.m[idx] = self.beta1 * self.m[idx] + (1 - self.beta1) * grad\n",
    "                self.v[idx] = self.beta2 * self.v[idx] + (1 - self.beta2) * (grad ** 2)\n",
    "                m_hat = self.m[idx] / (1 - self.beta1 ** self.t)\n",
    "                v_hat = self.v[idx] / (1 - self.beta2 ** self.t)\n",
    "                p.data -= m_hat / (np.sqrt(v_hat) + self.eps) * self.lr"
   ],
   "id": "37f9bbc9fe4b8820",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.489332Z",
     "start_time": "2025-09-07T03:04:24.477078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LSTM(Layer):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_size):\n",
    "        super().__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = Embedding(vocabulary_size, embedding_size)\n",
    "        self.forget_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.input_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.output_gate = Linear(embedding_size * 2, embedding_size)\n",
    "        self.cell_update = Linear(embedding_size * 2, embedding_size)\n",
    "        self.output = Linear(embedding_size, vocabulary_size)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.tanh = Tanh()\n",
    "\n",
    "        self.layers = [self.embedding,\n",
    "                       self.forget_gate,\n",
    "                       self.input_gate,\n",
    "                       self.output_gate,\n",
    "                       self.cell_update,\n",
    "                       self.output,\n",
    "                       self.sigmoid,\n",
    "                       self.tanh]\n",
    "\n",
    "    def __call__(self, x: Tensor, c: Tensor, h: Tensor):\n",
    "        return self.forward(x, c, h)\n",
    "\n",
    "    def forward(self, x: Tensor, c: Tensor, h: Tensor):\n",
    "        if not c:\n",
    "            c = Tensor(np.zeros((1, self.embedding_size)))\n",
    "        if not h:\n",
    "            h = Tensor(np.zeros((1, self.embedding_size)))\n",
    "\n",
    "        embedding_feature = self.embedding(x)\n",
    "        concat_feature = embedding_feature.concat(h, axis=1)\n",
    "        forget_hidden = self.sigmoid(self.forget_gate(concat_feature))\n",
    "        input_hidden = self.sigmoid(self.input_gate(concat_feature))\n",
    "        output_hidden = self.sigmoid(self.output_gate(concat_feature))\n",
    "        cell_hidden = self.tanh(self.cell_update(concat_feature))\n",
    "        cell_feature = forget_hidden * c + input_hidden * cell_hidden\n",
    "        hidden_feature = output_hidden * self.tanh(cell_feature)\n",
    "\n",
    "        return (self.output(hidden_feature),\n",
    "                Tensor(cell_feature.data),\n",
    "                Tensor(hidden_feature.data))\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for l in self.layers for p in l.parameters()]"
   ],
   "id": "f75cec5a90fb1bac",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:24.542450Z",
     "start_time": "2025-09-07T03:04:24.538208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "LEARNING_RATE = 0.02\n",
    "BATCHES = 2\n",
    "EPOCHS = 10"
   ],
   "id": "77704b9794631a8",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:41.099213Z",
     "start_time": "2025-09-07T03:04:24.592880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DataLoader(BATCHES)\n",
    "\n",
    "model = LSTM(len(dataset.vocabulary), 64)\n",
    "\n",
    "loss = CELoss()\n",
    "sgd = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(len(dataset)):\n",
    "        sequence = dataset[i]\n",
    "\n",
    "        cell = hidden = None\n",
    "        for i in range(len(sequence)):\n",
    "            feature, label = sequence[i]\n",
    "\n",
    "            prediction, cell, hidden = model(feature, cell, hidden)\n",
    "            error = loss(prediction, label)\n",
    "\n",
    "            error.gradient()\n",
    "            sgd.backward()\n",
    "\n",
    "print(f'Prediction: {prediction.data}')\n",
    "print(f'Error: {error.data}')"
   ],
   "id": "503895b6c99d059c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[-4.04900090e+00 -5.84674294e+00 -5.33842421e+00 -5.57115736e+00\n",
      "  -8.18867773e+00 -1.34329802e+01 -1.16146400e+01 -3.14296912e+00\n",
      "  -1.12861749e+01 -1.76461452e+01 -3.40336663e+00 -8.99788956e+00\n",
      "  -3.89741699e+00 -5.65133165e+00 -1.54284206e+00 -1.28098516e+01\n",
      "  -2.30106175e+00 -7.43985541e+00 -8.80805212e+00 -4.53431763e+00\n",
      "  -1.11425452e+01 -7.82262266e+00 -1.70105253e+01 -1.57685972e+01\n",
      "  -4.25978467e-01 -3.98345078e+00 -1.53478424e+01 -5.17304282e+00\n",
      "  -6.75999380e+00 -7.77051522e+00 -4.52281018e-02 -5.62405349e-01\n",
      "  -1.76404996e+01 -2.51374861e+00 -1.24502808e+01 -7.15392786e+00\n",
      "  -8.43083031e+00 -1.10636635e+01 -1.82611784e+00 -1.55500135e+01\n",
      "  -1.05460036e+01 -1.70282013e+01 -8.40966213e+00 -1.34897113e+00\n",
      "  -5.59147875e+00 -1.20092543e+01 -4.89029561e-03  2.05789977e+00\n",
      "  -7.29836026e+00 -1.54971561e+00 -1.84463199e+01 -9.22815772e+00\n",
      "  -1.31208494e+01 -5.30092067e+00 -1.60852403e+01 -1.05135157e+01\n",
      "  -5.92977257e+00 -9.08364434e+00 -1.01969223e+01 -2.02484418e+00\n",
      "  -9.86625957e+00 -1.72729679e+01 -8.37329929e+00 -1.61243607e+00\n",
      "   1.08478108e+00 -9.92506471e+00 -1.10964960e+01 -1.06615312e+01\n",
      "  -4.61238496e+00 -1.04236025e+01 -1.21746958e+01 -1.50019519e+01\n",
      "  -7.10825479e+00 -1.18488261e+01 -4.15883020e+00 -3.84460659e+00\n",
      "  -2.67069258e+00 -2.76278636e+00 -6.80383855e+00 -4.32782109e+00\n",
      "   1.46764545e+00 -5.60647111e+00 -5.06163800e+00 -1.21079144e+01\n",
      "  -1.38944293e+01 -1.01052932e+01]]\n",
      "Error: 1.9132558278492295\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-07T03:04:41.190989Z",
     "start_time": "2025-09-07T03:04:41.129336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset.eval()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sequence = dataset[i]\n",
    "\n",
    "    feature, label = sequence[0]\n",
    "    original = sequence.tokens[:BATCHES]\n",
    "    generated = original.copy()\n",
    "\n",
    "    cell = hidden = None\n",
    "    for j in range(len(sequence)):\n",
    "        feature, label = sequence[j]\n",
    "\n",
    "        prediction, cell, hidden = model(feature, cell, hidden)\n",
    "        original.append(sequence.tokens[j + BATCHES])\n",
    "        generated.append(prediction.data.argmax())\n",
    "\n",
    "    print(f'original: {dataset.decode(original)}')\n",
    "    print(f'generated: {dataset.decode(generated)}')"
   ],
   "id": "e29eb7d072ed8997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: worst movie with awful music the actor did a boring job actress director character screenplay scene but or it at by\n",
      "generated: worst movie with and and actor director and the the on the character character screenplay mine music effect if plot character\n",
      "original: i recommend this film the actress was wonderful and the music was amazing actor director character scene plot by was\n",
      "generated: i recommend this film the actor was enjoyed i screenplay story was actress director actress screenplay screenplay plot his story\n",
      "original: poor movie the story and actor were terrible i disappointed the director actress character action screenplay scene by is\n",
      "generated: poor movie and and and script actress actress i screenplay actress performance story plot plot music is plot is\n",
      "original: excellent film i saw this time perfect performance by the actor and actress director screenplay scene character his her\n",
      "generated: excellent film the saw this time fantastic the actor was was and the the disappointed visual plot plot i\n",
      "original: boring movie with poor effect the director did a awful job actor character actress screenplay music plot by was is\n",
      "generated: boring movie and and and actress actress did a enjoyed the actor character screenplay plot the music the story i\n",
      "original: this film was wonderful i loved the plot and actress the scene was amazing director character actor screenplay his her\n",
      "generated: this film was was and enjoyed the story and script story was was poor the perfect screenplay screenplay job i\n",
      "original: great movie the actor and screenplay were excellent i enjoyed the visual actress character scene script music effect or\n",
      "generated: great movie the and and the were actress actress actress the screenplay actor actress time plot effect was actress\n",
      "original: i waste of time the screenplay was boring and the scene was awful actor actress character screenplay music effect by\n",
      "generated: i waste time time the actor was awful effect effect story was his and director screenplay screenplay actress her it\n",
      "original: best film with fantastic soundtrack the director did a perfect job actor actress character scene screenplay music in on\n",
      "generated: best film screenplay was the the actor did a the and director director plot plot plot plot music plot\n",
      "original: this movie was terrible i hated the story and actor the scene was poor actress director character screenplay action by\n",
      "generated: this movie was i i screenplay the music and the was was was boring actress her plot screenplay actress is\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
